{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b132247d-6f04-5a90-1eec-7f0872a70a98"
   },
   "source": [
    "#Verify on UCI dataset\n",
    "1. Decision Tree Classifier/ Random Forest Classifier\n",
    "\n",
    "2. Logistic regression\n",
    "\n",
    "3. SVM\n",
    "\n",
    "4. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "73c46801-7f23-67a2-47e8-03256085f659",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "228eec75-5856-5411-7e59-a4d419c2e520"
   },
   "source": [
    "### Lets start with Importing Libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "7acc51d2-e9db-908f-9b91-386e23629483",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leshare/softwares/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # to import csv and for data manipulation\n",
    "import matplotlib.pyplot as plt # to plot graph\n",
    "import numpy as np # for linear algebra\n",
    "import datetime # to dela with date and time\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler # for preprocessing the data\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # for Decision Tree classifier\n",
    "from sklearn.svm import LinearSVC as SVC # for SVM classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split # to split the data\n",
    "from sklearn.cross_validation import KFold # For cross vbalidation\n",
    "from sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "83f83459-d9f6-e2fd-9d11-77ee19a48ff0",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/real/vehicle.dat\",header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8c79df42-9432-e39e-cb27-6c53245f9ac5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('percentage of normal transacation is', 74.34988179669031)\n",
      "('percentage of fraud transacation', 25.65011820330969)\n"
     ]
    }
   ],
   "source": [
    "# now let us check in the number of Percentage\n",
    "data['Class'][data['Class']=='positive'] = 1\n",
    "data['Class'][data['Class']=='negative'] = 0\n",
    "Count_Normal_transacation = len(data[data[\"Class\"]==0]) \n",
    "Count_Fraud_transacation = len(data[data[\"Class\"]==1]) \n",
    "Percentage_of_Normal_transacation = 1.*Count_Normal_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\n",
    "print(\"percentage of normal transacation is\",Percentage_of_Normal_transacation*100)\n",
    "Percentage_of_Fraud_transacation= 1.*Count_Fraud_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\n",
    "print(\"percentage of fraud transacation\",Percentage_of_Fraud_transacation*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "12f3b2ed-4231-cddc-9959-d13491af2497",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, auc, f1_score, precision_recall_fscore_support\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "def evaluate_auc_prc(y, pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y, pred)\n",
    "    aucprc = auc(recall, precision)\n",
    "    #print 'AUPRC:{}'.format(aucprc)\n",
    "    #plt.title('Precision Recall Curve')\n",
    "    #plt.plot(precision, recall, 'b',label='AUC = %0.2f'% aucprc)\n",
    "    #plt.legend(loc='lower right')\n",
    "    #plt.ylabel('Precision')\n",
    "    #plt.xlabel('Recall')\n",
    "    #plt.show()\n",
    "    return aucprc\n",
    "\n",
    "\n",
    "def evaluate_f1(y, y_pred, pos_label=1):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y, y_pred, pos_label=1)\n",
    "    #print classification_report(y, y_pred)\n",
    "    return f1[1]\n",
    "\n",
    "aucprc_score = make_scorer(evaluate_auc_prc, greater_is_better=True)\n",
    "\n",
    "gms = make_scorer(geometric_mean_score, greater_is_better=True)\n",
    "\n",
    "f1_scorer = make_scorer(evaluate_f1, greater_is_better=True)\n",
    "\n",
    "def data_split(x, test_size=0.4, seed=1): # preparing data for training and testing as we are going to use different data \n",
    "    #again and again so make a function\n",
    "    x_features= x.ix[:,x.columns != \"Class\"]\n",
    "    x_labels=x.ix[:,x.columns==\"Class\"]\n",
    "    x_features_train,x_features_test,x_labels_train,x_labels_test = train_test_split(x_features,x_labels,\n",
    "                                                                                     test_size=test_size, \n",
    "                                                                                     random_state=seed)\n",
    "    x_features_train.index = np.arange(len(x_features_train))\n",
    "    x_labels_train.index = np.arange(len(x_labels_train))\n",
    "    x_features_test.index = np.arange(len(x_features_test))\n",
    "    x_labels_test.index = np.arange(len(x_labels_test))\n",
    "    print(\"length of training data\")\n",
    "    print(len(x_features_train))\n",
    "    print(\"length of test data\")\n",
    "    print(len(x_features_test))\n",
    "    return(x_features_train,x_features_test,x_labels_train,x_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8c5529ce-71ca-b24b-b544-e3d1443115f9",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# before starting we should standridze \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "y = data['Class'].values\n",
    "X = data.drop(['Class'], axis=1).values\n",
    "scalar = RobustScaler()\n",
    "X = scalar.fit_transform(X)\n",
    "x_train = X\n",
    "y_train = y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'x':x_train, 'y':np.eye(2)[y_train.astype('int32')]}, open('../data/real/vehicle/train.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import LinearSVC as SVM\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_c for MLP:5\n",
      "best_reward MLP:0.685563975135\n"
     ]
    }
   ],
   "source": [
    "# Choose classifer with cross validation\n",
    "# Supervised Model: LR, SVM (linear), SVM (rbf), DT, KNN, MLP\n",
    "# Dataset: original\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# LR\n",
    "# best_c = 0\n",
    "# best_reward = 0\n",
    "# for c in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]:\n",
    "#     clf = LR(C=c, random_state=0)\n",
    "#     reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "#     if reward > best_reward:\n",
    "#         best_reward = reward\n",
    "#         best_c = c\n",
    "# print 'best_c for LR:{}'.format(best_c)\n",
    "# print 'best_reward for LR:{}'.format(best_reward)\n",
    "\n",
    "# SVM (linear)\n",
    "# best_c = 0\n",
    "# best_reward = 0\n",
    "# for c in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]:\n",
    "#     clf = SVM(C=c, random_state=0)\n",
    "#     reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "#     if reward > best_reward:\n",
    "#         best_reward = reward\n",
    "#         best_c = c\n",
    "# print 'best_c for SVM:{}'.format(best_c)\n",
    "# print 'best_reward for SVM:{}'.format(best_reward)\n",
    "\n",
    "# SVM (rbf)\n",
    "# best_c = 0\n",
    "# best_reward = 0\n",
    "# for c in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]:\n",
    "#     clf = SVC(C=c, kernel='rbf', random_state=0)\n",
    "#     reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "#     if reward > best_reward:\n",
    "#         best_reward = reward\n",
    "#         best_c = c\n",
    "# print 'best_c for SVC:{}'.format(best_c)\n",
    "# print 'best_reward for SVC:{}'.format(best_reward)\n",
    "\n",
    "# DT \n",
    "# best_c = 0\n",
    "# best_reward = 0\n",
    "# for c in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "#     clf = DT(max_depth=c, random_state=0)\n",
    "#     reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "#     if reward > best_reward:\n",
    "#         best_reward = reward\n",
    "#         best_c = c\n",
    "# print 'best_c for DT:{}'.format(best_c)\n",
    "# print 'best_reward DT:{}'.format(best_reward)\n",
    "\n",
    "# KNN \n",
    "# best_c = 0\n",
    "# best_reward = 0\n",
    "# for c in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "#     clf = KNN(n_neighbors=c)\n",
    "#     reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "#     if reward > best_reward:\n",
    "#         best_reward = reward\n",
    "#         best_c = c\n",
    "# print 'best_c for KNN:{}'.format(best_c)\n",
    "# print 'best_reward KNN:{}'.format(best_reward)\n",
    "\n",
    "# KNN \n",
    "best_c = 0\n",
    "best_reward = 0\n",
    "for c in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    clf = MLP(hidden_layer_sizes=[5, c], random_state=0)\n",
    "    reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_c = c\n",
    "print 'best_c for MLP:{}'.format(best_c)\n",
    "print 'best_reward MLP:{}'.format(best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:100.0\n",
      "best_c:100.0\n",
      "best_reward:0.803499737701\n"
     ]
    }
   ],
   "source": [
    "# Choose hyper-parameters with cross validation\n",
    "# Supervised Model: SVM\n",
    "# Dataset: original\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "best_c = 0\n",
    "best_reward = 0\n",
    "for c in [1e2]:\n",
    "    print('c:{}'.format(c))\n",
    "    clf = SVC(C=c, kernel='rbf', random_state=0)\n",
    "    reward = np.array(cross_val_score(clf, x_train, y_train, cv=5, n_jobs=5, scoring=gms)).mean()\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_c = c\n",
    "print 'best_c:{}'.format(best_c)\n",
    "print 'best_reward:{}'.format(best_reward)\n",
    "clf = SVC(C=best_c, kernel='rbf', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward on original dataset:0.935698711428\n"
     ]
    }
   ],
   "source": [
    "# Performance on original dataset with the chosen model.\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_train)\n",
    "reward = geometric_mean_score(y_train, preds)\n",
    "print 'reward on original dataset:{}'.format(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best under-sampling ratio with Random:2.0\n",
      "best reward with Random:0.949336798322\n",
      "best under-sampling ratio with ENN:1.0\n",
      "best reward with ENN:0.877197445112\n",
      "best under-sampling ratio with Cluster:1.4\n",
      "best reward with Cluster:0.936653466352\n",
      "best under-sampling ratio with TomekLinks:1.0\n",
      "best reward with TomekLinks:0.938146254243\n",
      "best under-sampling ratio with ALLKNN:1.0\n",
      "best reward with ALLKNN:0.857956310197\n"
     ]
    }
   ],
   "source": [
    "# Choose under-sampling ratio for different under-sampling methods\n",
    "# Supervised Model: SVM(C=1e2, kernel='rbf', random_state=0)\n",
    "# Under sampling methods: Random, ENN, Cluster, TomekLinks, ALLKNN\n",
    "from imblearn.under_sampling import (NearMiss, RandomUnderSampler, EditedNearestNeighbours, \n",
    "                                     CondensedNearestNeighbour, ClusterCentroids, TomekLinks,\n",
    "                                     RepeatedEditedNearestNeighbours, AllKNN)\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "train_x = x_train\n",
    "train_y = y_train\n",
    "pos_num = (train_y == 1).sum()\n",
    "neg_num = (train_y == 0).sum()\n",
    "\n",
    "\n",
    "for sampler, name, time in zip([RandomUnderSampler, EditedNearestNeighbours, ClusterCentroids, TomekLinks, AllKNN],\n",
    "                         ['Random', 'ENN', 'Cluster', 'TomekLinks', 'ALLKNN'], [50, 1, 1, 1, 1]):\n",
    "    max_i = 1\n",
    "    best_reward = -1\n",
    "    for i in np.arange(1, 1.0*neg_num/pos_num, 0.2):\n",
    "        sample = sampler(ratio={0:int(i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "            max_i = i\n",
    "    print 'best under-sampling ratio with {}:{}'.format(name, max_i)\n",
    "    \n",
    "    best_reward = 0\n",
    "    for i in np.arange(time):\n",
    "        sample = sampler(ratio={0:int(max_i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "    print 'best reward with {}:{}'.format(name, best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best under-sampling ratio for Random:2.2\n",
      "best reward with Random:0.951518307469\n"
     ]
    }
   ],
   "source": [
    "# Choose under-sampling ratio for different under-sampling methods\n",
    "# Supervised Model: SVM(C=1e2, kernel='rbf', random_state=0)\n",
    "# Under sampling methods: Random, ENN, Cluster, TomekLinks, ALLKNN\n",
    "from imblearn.under_sampling import (NearMiss, RandomUnderSampler, EditedNearestNeighbours, \n",
    "                                     CondensedNearestNeighbour, ClusterCentroids, TomekLinks,\n",
    "                                     RepeatedEditedNearestNeighbours, AllKNN)\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "clf = SVC(C=1e2, kernel='rbf', random_state=0)\n",
    "train_x = x_train\n",
    "train_y = y_train\n",
    "pos_num = (train_y == 1).sum()\n",
    "neg_num = (train_y == 0).sum()\n",
    "\n",
    "\n",
    "for sampler, name, time in zip([RandomUnderSampler],\n",
    "                         ['Random'], [50, 1, 1, 1, 1]):\n",
    "    max_i = 1\n",
    "    best_reward = -1\n",
    "    for i in np.arange(1, 1.0*neg_num/pos_num, 0.2):\n",
    "        sample = sampler(ratio={0:int(i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "            max_i = i\n",
    "    print 'best under-sampling ratio for {}:{}'.format(name, max_i)\n",
    "    \n",
    "    best_reward = 0\n",
    "    for i in np.arange(time):\n",
    "        sample = sampler(ratio={0:int(max_i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "    print 'best reward with {}:{}'.format(name, best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[ 0  0 10  0  0  0  0  0  0 10]\n",
      "best under-sampling ratio for EasyEnsemble:2.0\n",
      "best reward for EasyEnsemble:0.952753304063\n"
     ]
    }
   ],
   "source": [
    "# Perform EasyEnsemble and BalanceCascade\n",
    "from imblearn.ensemble import EasyEnsemble, BalanceCascade\n",
    "\n",
    "clf = SVC(C=1e2, kernel='rbf', random_state=0)\n",
    "train_x = x_train\n",
    "train_y = y_train\n",
    "pos_num = (train_y == 1).sum()\n",
    "neg_num = (train_y == 0).sum()\n",
    "\n",
    "for sampler, name in zip([EasyEnsemble],\n",
    "                         ['EasyEnsemble']):\n",
    "    max_i = 1\n",
    "    best_reward = -1\n",
    "    for i in [2.0]:\n",
    "        sample = sampler(ratio={0:int(i*pos_num)}, replacement=False, n_subsets=10)\n",
    "        train_x_s, train_y_s = sample.fit_sample(x_train, y_train)\n",
    "        preds = None\n",
    "        print(len(train_x_s))\n",
    "        for x, y in zip(train_x_s, train_y_s):\n",
    "            clf.fit(x, y)\n",
    "            if preds is None:\n",
    "                preds = clf.predict(train_x)\n",
    "            else:\n",
    "                preds += clf.predict(train_x)\n",
    "        print(preds[:10])\n",
    "        preds = (preds >= ((len(train_x_s)+1)/2)).astype('int32')\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "            max_i = i\n",
    "    print 'best under-sampling ratio for {}:{}'.format(name, max_i)\n",
    "    print 'best reward for {}:{}'.format(name, best_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best over-sampling ratio with Random:2.5\n",
      "best reward with Random:0.965213717201\n",
      "best over-sampling ratio with SMOTE:2.5\n",
      "best reward with SMOTE:0.966268210363\n",
      "best over-sampling ratio with ADASYN:2.5\n",
      "best reward with ADASYN:0.966859436584\n"
     ]
    }
   ],
   "source": [
    "# Choose over-sampling ratio for different orver-sampling methods\n",
    "# Supervised Model: SVM(C=1e2, kernel='rbf', random_state=0)\n",
    "# Over sampling methods: Random, SMOTE, ADASYN\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "for sampler, name, time in zip([RandomOverSampler, SMOTE, ADASYN],\n",
    "                         ['Random', 'SMOTE', 'ADASYN'], [50, 10, 10]):\n",
    "    max_i = 1\n",
    "    best_reward = -1\n",
    "    for i in np.arange(1, 1.0*neg_num/pos_num, 0.5):\n",
    "        sample = sampler(ratio={1:int(i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward    \n",
    "            max_i = i\n",
    "    print 'best over-sampling ratio with {}:{}'.format(name, max_i)\n",
    "    \n",
    "    best_reward = 0\n",
    "    bset_set = None\n",
    "    for i in np.arange(time):\n",
    "        sample = sampler(ratio={1:int(max_i*pos_num)})\n",
    "        train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "        clf.fit(train_x_s, train_y_s)\n",
    "        preds = clf.predict(train_x)\n",
    "        reward = geometric_mean_score(train_y, preds)\n",
    "        if best_reward < reward:\n",
    "            best_reward = reward  \n",
    "            bset_set = (train_x_s, train_y_s)\n",
    "    #pickle.dump({'x':bset_set[0], 'y':np.eye(2)[bset_set[1].astype('int32')]}, open('../data/real/vehicle/train_{}.pkl'.format(name), 'wb+'))\n",
    "    print 'best reward with {}:{}'.format(name, best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'x':train_x, 'y':np.eye(2)[train_y.astype('int32')]}, open('../data/real/vehicle/train.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test performance of over-sampling methods with chosen ratio\n",
    "# Supervised Model: SVM(C=1e2, kernel='rbf', random_state=0)\n",
    "# Over sampling methods: Random, SMOTE, ADASYN\n",
    "best_reward = 0\n",
    "sampler = ADASYN(ratio={0:int(max_i*pos_num)})\n",
    "\n",
    "for i in np.range(50):\n",
    "    pos_num = (train_y == 1).sum()\n",
    "    neg_num = (train_y == 0).sum()\n",
    "    train_x_s, train_y_s = sampler.fit_sample(train_x, train_y)\n",
    "    clf.fit(train_x_s, train_y_s)\n",
    "    preds = clf.predict(train_x)\n",
    "    reward = geometric_mean_score(train_y, preds)\n",
    "    if best_reward < reward:\n",
    "        best_reward = reward    \n",
    "print 'best reward:{}'.format(best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 434, 18)\n",
      "(10, 434)\n",
      "best reward with EasyEnsemble:0.935945319289\n",
      "(4, 434, 18)\n",
      "(4, 434)\n",
      "best reward with BalanceCascade:0.0\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "# Supervised Model: SVM(C=1e2, kernel='rbf', random_state=0)\n",
    "# Ensemble methods: EasyEnsemble, BalanceCasecade\n",
    "from imblearn.ensemble import BalanceCascade, EasyEnsemble\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "train_x = x_train\n",
    "train_y = y_train\n",
    "pos_num = (train_y == 1).sum()\n",
    "neg_num = (train_y == 0).sum()\n",
    "\n",
    "\n",
    "for sampler, name, time in zip([EasyEnsemble, BalanceCascade],\n",
    "                         ['EasyEnsemble', 'BalanceCascade'], [50, 1, 1, 1, 1]):\n",
    "    \n",
    "    sample = sampler(n_subsets=10)\n",
    "    train_x_s, train_y_s = sample.fit_sample(train_x, train_y)\n",
    "    print(train_x_s.shape)\n",
    "    print(train_y_s.shape)\n",
    "    preds = np.zeros(len(train_y))\n",
    "    for i in range(len(train_x_s)):\n",
    "        clf.fit(train_x_s[i], train_y_s[i])\n",
    "        preds += clf.predict(train_x)\n",
    "    preds = (preds > 5).astype('int32')\n",
    "    reward = geometric_mean_score(train_y, preds)\n",
    "    print 'best reward with {}:{}'.format(name, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "651a09d1-bc75-e3bf-7c7f-743b832bbd05"
   },
   "source": [
    "### MLP for multiply operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse:2.09370660105e-05\n",
      "2.13083930475e-05\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100000, 2)\n",
    "y = x[:, 0] * x[:, 1]\n",
    "np.random.seed = 1234\n",
    "x_test = np.random.rand(20000, 2)\n",
    "y_test = x_test[:, 0] * x_test[:, 1]\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rg = MLPRegressor(hidden_layer_sizes=(100, ), activation='tanh')\n",
    "preds = rg.fit(x, y).predict(x)\n",
    "mse = mean_squared_error(y, preds)\n",
    "print 'train mse:{}'.format(mse)\n",
    "preds = rg.predict(x_test)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "error = (y_test-preds)\n",
    "print mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.abs(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0298732262179\n",
      "6.23440807701e-07\n",
      "0.00344771853463\n"
     ]
    }
   ],
   "source": [
    "print error.max()\n",
    "print error.min()\n",
    "print error.mean()"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
